{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cef9405cb63c6fe659d94b9e8f81730ec8c1b85"
      },
      "cell_type": "code",
      "source": "#TSB\n#import tools required to run the code\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport os\nimport pandas as pd \nimport requests\nimport logging\nimport time\nimport seaborn as sns\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ffbd584db309e9f52b58cfd7a954db430418e011"
      },
      "cell_type": "markdown",
      "source": "**Import the two related files in the dataset**\n\n* Inspections file - contains all the inspections from the county inspectors\n* Violations file - contains details of violations found - restaurant names, type of violation etc."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eafdf879b59c1eb9d45c7df15c674c1b4dd8f63"
      },
      "cell_type": "code",
      "source": "#nRowsRead = 100  - use this if you only want to test the waters and not import the whole file, remember to uncomment until the hyphen :)\n#read in file with all inspections and put in dataframe\ndf1 = pd.read_csv('../input/restaurant-and-market-health-inspections.csv', delimiter=',')\nnRow, nCol = df1.shape\n#print number of rows and columns\nprint({nRow}, {nCol})\n\n#read in file with all health VIOLATIONS and put in dataframe\ndf2 = pd.read_csv('../input/restaurant-and-market-health-violations.csv')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "c3e41d91abb86c3add0c6e121f99bb344a23af81"
      },
      "cell_type": "code",
      "source": "from matplotlib import reload\nreload(plt)\n%matplotlib notebook\ndf1['score']=df1['score'].astype(int) #make score of type INTEGER\n\nscore_dist_hist = df1['score'].plot(kind = \"hist\", title = \"Score Distribution\",  figsize = (7,5), alpha = 0.5)\nscore_dist_hist.set_xlabel(\"score - values\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b64bda330d184396aa32dfc67995e2ee7fb1078a"
      },
      "cell_type": "code",
      "source": "df2['score'].hist().plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3546a3720fab56b8724d519107970b72624db94d"
      },
      "cell_type": "code",
      "source": "#search for missing grade, i.e, that is not A or B or C\ndf1[~df1['grade'].isin(['A','B','C'])]\n#one record (record # 49492) popped up, so we fix it by assigning it a grade C\ndf1.loc[49492,['grade']] = 'C'\n\n#find out the distribution of the grades, i.e, how many records in each grade\n#basically, group dataframe column grade by grade letter, then count and index on that, and print it\ngrade_distribution = df1.groupby('grade').size()\npd.DataFrame({'Count of restaurant grade totals':grade_distribution.values}, index=grade_distribution.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b21d982aea607d5de26ca9a53549dbbfe3f375c0"
      },
      "cell_type": "markdown",
      "source": "#### Note that the totals above add up to 58,872, which is the total # of rows in the file, which means there are no rows without a grade."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41779caaa05082fccbc1527d2aa9e4ec80af9f47"
      },
      "cell_type": "code",
      "source": "#print the top 10 rows of the violations dataframe (df2)\ndf2.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "972197cb571ad5bbca9b21c98c8911c0d81ef88d"
      },
      "cell_type": "code",
      "source": "#group by restaurant type and count # in each category\n#then sort from highest # to lowest, then create a bar graph\ntemp = df1.groupby('pe_description').size()\ndescription_distribution = pd.DataFrame({'Count':temp.values}, index=temp.index)\ndescription_distribution = description_distribution.sort_values(by=['Count'], ascending=True)\ndf2['pe_description'].hist().plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a91407c49b0a0a6349999a7265240303bbd6456d"
      },
      "cell_type": "code",
      "source": "#the previous charts and graphs show breakdown of various types food restaurants with risk\n#broken down to high, medium, low.\n#This procedure use the split function to break the pe_description field into the sub-string\n#after the 2nd space from the end - ex: string x = \"Aa Bb Cc\", when split applied like this: x.split(' ')[-2] ->sub-string after(not before) 2nd space '=>Bb'\ndef sub_risk(x):\n    return x.split(' ')[-2]\n\ndf2['risk'] = df2['pe_description'].astype(str).apply(sub_risk)         #apply function to get only high, medium, low\ntemp = df2.groupby('risk').size()                                       #group, count by risk level\n#plot the histogram for the 3 levels of risk\ndf2['risk'].hist().plot()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3545abe1749fc5808e3e5531586b4d410f864627"
      },
      "cell_type": "code",
      "source": "#calculate and plot pie chart for risk\nrisk_distribution = pd.DataFrame({'Count':temp.values}, index = temp.index)\nrisk_distribution\nax2 = risk_distribution['Count'].plot(kind=\"pie\", legend=True,autopct='%.2f', figsize=(6, 6))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "52503192ab6dd68d7fd2417e3c2e5851ac05b77c"
      },
      "cell_type": "code",
      "source": "#show first 10 rows of the violations file dataframe \ndf2.head(10)\n#groupb by violation_description, count and sort them from largest violation by count to smallest\nviolation_description = df2.groupby('violation_description').size()\npd.DataFrame({'Count':violation_description.values},index = violation_description.index).sort_values(by = 'Count',ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1717d7670d34a34e1ff34a94307534f81580c8d"
      },
      "cell_type": "code",
      "source": "df2['pe_description'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9bace006489c8c63e9080ad183cf32d3ad54337"
      },
      "cell_type": "code",
      "source": "#create a simple proc for heat map for risk - low, moderate, high\ndef convert_risk_value(x):\n    if x == 'LOW':\n        return 10\n    elif x == 'MODERATE':\n        return 5\n    else:\n        return 0\n#create simple proc to map grade to value    \ndef convert_grade_value(x):\n    if x == 'A':\n        return 10\n    elif x == 'B':\n        return 5\n    else:\n        return 0\n#call (apply) procs created above    \ndf2['risk_value']=df2['risk'].apply(convert_risk_value)\ndf2['grade_value']=df2['grade'].apply(convert_grade_value)\ndf3 = df2.loc[:,['score', 'grade_value', 'risk_value']]\ncorr = df3.corr()\ncorr = (corr)\nsns.heatmap(corr, xticklabels = corr.columns.values, yticklabels=corr.columns.values, cmap=\"Purples\", center=0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0aa7ca63c99005002a5183152fc60743971b8fcf"
      },
      "cell_type": "code",
      "source": "#top 20 facilities with most restaurants / markets\nfacility_dist = df1.groupby(['facility_id', 'facility_name']).size()\ntop20_facilities = facility_dist.sort_values(ascending=False).head(20)\npd.DataFrame({'Count':top20_facilities.values}, index=top20_facilities.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "d7818853c0e85cd55d8863b361ebcb0009891241"
      },
      "cell_type": "code",
      "source": "#top 30 owners with most restaurants / markets\nowner_dist = df1.groupby(['owner_id', 'owner_name']).size()\ntop30_owners = owner_dist.sort_values(ascending=False).head(30)\npd.DataFrame({'Count':top30_owners.values}, index= top30_owners.index)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "147d575621e88814e14f5ba3f865fd80d153757d"
      },
      "cell_type": "code",
      "source": "#violations listing from most common, descending - violation description, violation code, counts\nviolation_desc=df2.groupby(['violation_description','violation_code']).size()\npd.DataFrame({'Count':violation_desc.values}, index=violation_desc.index).sort_values(by = 'Count', ascending=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9074f925579d70fbfc483eb3bedab52166c3cff"
      },
      "cell_type": "code",
      "source": "#list facilities with most violations and type of violation\n#create a dataframe with facility and violation columns, aggregate by size, then count and sort them\nviolation_desc2 = df2.groupby(['facility_name','violation_description']).size()\npd.DataFrame({'Count':violation_desc2.values}, index=violation_desc2.index).sort_values(by='Count', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "560a88bfade638080129e9e4f1d2cb48457a3add",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "df1.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e946cbfafc7a3336ef5ac522e6a09d0df3a1800a"
      },
      "cell_type": "code",
      "source": "#get a list of all the restaurants with grade C\ndf4 = df2.loc[(df2['grade'] == 'C'),['facility_name','facility_address','facility_zip']]\ndf4=df4.drop_duplicates(['facility_name'])  #only want each restaurant listed once, since many of them have multiple violations\ndf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3e51cc6b901a0808768c25bb2ac4dc686b41d45"
      },
      "cell_type": "code",
      "source": "#visualize bad restaurants (grade C)on a map, so that if you are in that area, you can avoid them :)\n#some of them might have remediated their violations, or may be operating under \"new management\" or maybe even turned over a new leaf - we just don't know\naddresses_to_avoid = df4['facility_address'].tolist()\naddresses_to_avoid = (df4['facility_name'] + ',' + df4['facility_address'] + ',' + 'LOS ANGELES' + ',CA').tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f42234c8a66479225cb9100b4d1f77f1c521932"
      },
      "cell_type": "code",
      "source": "print(addresses_to_avoid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "319287c0dced93f1cae9c593a8e77ab4adaed54a"
      },
      "cell_type": "code",
      "source": "#filter out by specific violation - in this case I picked Impoundment of unsanitary equipment or food - and list violators\ndf2.loc[(df2['violation_description'] == \"# 50. Impoundment of unsanitary equipment or food\") & (df2['violation_status']==\"OUT OF COMPLIANCE\"), ['facility_name','facility_address', 'violation_description','activity_date']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54206264e702e7ca27cd1246dcb7c990267056fa"
      },
      "cell_type": "code",
      "source": "addresses_for_violation_50= df2.loc[(df2['violation_description'] == \"# 50. Impoundment of unsanitary equipment or food\") & (df2['violation_status']==\"OUT OF COMPLIANCE\"), ['facility_name','facility_address']]\nprint(addresses_for_violation_50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1faa824918c9abb2c12d1516aa6b002627f7fff6"
      },
      "cell_type": "code",
      "source": "#this cell creates and prints breakdown list of violators for each violation code\n#first, get a unique list of violations -  .unique()\n#then, use a for loop to go from index 0 to the max number of items [index] for violation codes\n#for each unique violation, get a list of offenders\ntemp1 = df2.violation_description.unique()\nprint(len(temp1))                          #print the total number of UNIQUE violations in the database\nfor i in range(0,len(temp1)):              #for each one, get the list of restaurants that match it in dataset\n    temp2 = df2.loc[(df2['violation_description'] == temp1[i]) & (df2['violation_status']==\"OUT OF COMPLIANCE\"), ['violation_code','facility_name','facility_address']]\n    print(temp2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea99fec71d07968306b9c5488e437d297bb0bb06"
      },
      "cell_type": "code",
      "source": "# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        printf('No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title('Correlation Matrix for {filename}', fontsize=15)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7d2e5cccc517909fbccc6f50f49346aea1818e4"
      },
      "cell_type": "code",
      "source": "# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48cdb954178739f948b66b6033783847b3149621"
      },
      "cell_type": "code",
      "source": "df1.dataframeName = 'restaurant-and-market-health-inspections.csv'\ndf2.dataframeName = 'restaurant-and-market-health-violations.csv'\nplotCorrelationMatrix(df2, 8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69780fc6274fd2dd78a622f9ae46f634427e1a76"
      },
      "cell_type": "code",
      "source": "plotScatterMatrix(df1, 9, 10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79021c7762974b9805abe827581fdf36dcce1058"
      },
      "cell_type": "code",
      "source": "address_column_name = 'facility_address'\nrestaurant_name = 'facility_name'\nRETURN_FULL_RESULTS = False\nBACKOFF_TIME = 30",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "679690890a890f658dd7ec9633632abf58a78dce"
      },
      "cell_type": "code",
      "source": "#check to ensure that address column exists in dataset, else raise error\n#if address_column_name not in df1.columns:\n#    raise ValueError(\"missing Address column in input data\")\n    \n#take the addressese from the dataframe, add city and state and put in a list named 'addresses'\n#addresses = df1[address_column_name].tolist()\n#addresses = (df1[restaurant_name] + ',' + df1[address_column_name] + ',' + 'LOS ANGELES' + ',CA').tolist()\n#print(addresses)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be0a866a3217853400f25683b24a8acc42e926a"
      },
      "cell_type": "code",
      "source": "logger = logging.getLogger(\"root\")\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()      #console handler\nch.setLevel(logging.DEBUG)\nlogger.addHandler(ch)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef1bba36e7e0825151bf1d34e103f522fea7c06b"
      },
      "cell_type": "code",
      "source": "API_KEY = 'input your mapbox API key here'\noutput_filename = '../output/output-2018.csv'\nprint(addresses)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec3a68e5d027d287631bf62c138fafed63d2dba8"
      },
      "cell_type": "code",
      "source": "def get_google_results(address, api_key=None, return_full_response=False):\n    geocode_url = \"https://maps.googleapis.com/maps/api/geocode/json?address={}\".format(address)\n    if api_key is not None:\n        geocode_url = geocode_url + \"&key={}\".format(api_key)\n        \n        #ping google for the results:\n        results = requests.get(geocode_url)\n        results = results.json()             \n        \n        if len(results['results']) == 0:\n            output = {\n                \"formatted_address\" : None,\n                \"latitude\": None,\n                \"longitude\": None,\n                \"accuracy\": None,\n                \"google_place_id\": None,\n                \"type\": None,\n                \"postcode\": None\n            }\n        else:\n            answer = results['results'][0]\n            output = {\n                \"formatted_address\" : answer.get('formatted_address'),\n                \"latitude\": answer.get('geometry').get('location').get('lat'),\n                \"longitude\": answer.get('geometry').get('location').get('lng'),\n                \"accuracy\": answer.get('geometry').get('location_type'),\n                \"google_place_id\": answer.get(\"place_id\"),\n                \"type\": \",\".join(answer.get('types')),\n                \"postcode\": \",\".join([x['long_name'] for x in answer.get('address_components') \n                                  if 'postal_code' in x.get('types')])\n            }\n            \n        #append some other details\n        output['input_string'] = address\n        output['number_of_results'] = len(results['results'])\n        output['status'] = results.get('status')\n        if return_full_response is True:\n            output['response'] = results\n        \n        return output\n                ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90ea419d2737c162587c1efca17dcc5e712f9ed7"
      },
      "cell_type": "code",
      "source": "#test that API key validity, internet access confirmation, and function result are all good\ntest_result = get_google_results(\"HABITAT COFFEE SHOP, 3708 N EAGLE ROCK BLVD​, LOS ANGELES, CA\", API_KEY, RETURN_FULL_RESULTS)\nprint(test_result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "485a6ac584680af869c309a72f3e10778da81ede"
      },
      "cell_type": "code",
      "source": "#results=[]\n#for address in addresses:\n#    geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)\n#    results.append(geocode_result)\n\n#    pd.DataFrame(results).to_csv(output_filename, encoding='utf8')\n    \n    #print(geocode_result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e65a039b40341e803636392dbb29c22fe14c62df"
      },
      "cell_type": "code",
      "source": "results2=[]\nfor address in addresses_to_avoid:\n    geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)\n    results2.append(geocode_result)\n\n    pd.DataFrame(results2).to_csv('../input/restaurants_to_avoid.csv', encoding='utf8')\n    ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}